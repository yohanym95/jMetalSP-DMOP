package org.uma.jmetalsp.examples.dynamictsp;import org.apache.kafka.common.serialization.IntegerDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.streaming.Duration;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.uma.jmetal.operator.CrossoverOperator;import org.uma.jmetal.operator.MutationOperator;import org.uma.jmetal.operator.SelectionOperator;import org.uma.jmetal.operator.impl.crossover.PMXCrossover;import org.uma.jmetal.operator.impl.mutation.PermutationSwapMutation;import org.uma.jmetal.operator.impl.selection.BinaryTournamentSelection;import org.uma.jmetal.qualityindicator.impl.InvertedGenerationalDistance;import org.uma.jmetal.solution.PermutationSolution;import org.uma.jmetal.util.comparator.RankingAndCrowdingDistanceComparator;import org.uma.jmetal.util.point.PointSolution;import org.uma.jmetalsp.DataConsumer;import org.uma.jmetalsp.DynamicAlgorithm;import org.uma.jmetalsp.DynamicProblem;import org.uma.jmetalsp.JMetalSPApplication;import org.uma.jmetalsp.algorithm.nsgaii.DynamicNSGAIIBuilder;import org.uma.jmetalsp.consumer.ChartConsumer;import org.uma.jmetalsp.consumer.LocalDirectoryOutputConsumer;import org.uma.jmetalsp.observeddata.AlgorithmObservedData;import org.uma.jmetalsp.observeddata.ObservedValue;import org.uma.jmetalsp.observer.impl.DefaultObservable;import org.uma.jmetalsp.problem.tsp.MultiobjectiveTSPBuilderFromNYData;import org.uma.jmetalsp.problem.tsp.MultiobjectiveTSPBuilderFromTSPLIBFiles;import org.uma.jmetalsp.problem.tsp.TSPMatrixData;import org.uma.jmetalsp.qualityindicator.CoverageFront;import org.uma.jmetalsp.spark.SparkRuntime;import org.uma.jmetalsp.spark.streamingdatasource.SimpleSparkStructuredKafkaStreamingTSP;import java.util.HashMap;import java.util.List;import java.util.Map;/** * Example of SparkSP application. * Features: * - Algorithm: to choose among NSGA-II and MOCell * - Problem: Bi-objective TSP * - Default streaming runtime (Spark is not used) * * @author Antonio J. Nebro <antonio@lcc.uma.es> */public class DynamicTSPApplicationSparkKafka {  public static void main(String[] args) throws Exception {//    if (args.length != 1) {//      throw new Exception("Invalid number of arguments. " +//          "Spark home directory needed") ;//    }        String sparkHomeDirectory = "C:\\Spark\\bin" ;        SparkConf sparkConf = new SparkConf()            .setAppName("SparkApp")            .setSparkHome(sparkHomeDirectory)            .setMaster("local[4]")            .set("spark.streaming.kafka.consumer.cache.enabled", "false");        // STEP 1. Create the problem    DynamicProblem<PermutationSolution<Integer>, ObservedValue<TSPMatrixData>> problem;    problem = new MultiobjectiveTSPBuilderFromTSPLIBFiles("E:\\study files\\spark\\test\\jMetalSP-master\\data\\kroA100.tsp", "E:\\study files\\spark\\test\\jMetalSP-master\\data\\kroB100.tsp")           .build();   // problem = new MultiobjectiveTSPBuilderFromNYData("E:\\study files\\spark\\test\\jMetalSP-master\\data\\nyData.txt").build() ;    // STEP 2. Create the algorithm    CrossoverOperator<PermutationSolution<Integer>> crossover;    MutationOperator<PermutationSolution<Integer>> mutation;    SelectionOperator<List<PermutationSolution<Integer>>, PermutationSolution<Integer>> selection;    crossover = new PMXCrossover(0.9);    double mutationProbability = 0.2;    mutation = new PermutationSwapMutation<Integer>(mutationProbability);    selection = new BinaryTournamentSelection<>(        new RankingAndCrowdingDistanceComparator<PermutationSolution<Integer>>());    InvertedGenerationalDistance<PointSolution> igd =        new InvertedGenerationalDistance<>();    CoverageFront<PointSolution> coverageFront = new CoverageFront<>(0.005,igd);    DynamicAlgorithm<List<PermutationSolution<Integer>>, AlgorithmObservedData> algorithm;        algorithm = new DynamicNSGAIIBuilder<>(crossover, mutation, new DefaultObservable<>(),coverageFront)        .setMaxEvaluations(400000)        .setPopulationSize(100)        .setSelectionOperator(selection)        .build(problem);    // STEP 3. Create the streaming data source and register the problem    String topic="tsp";    Map<String,Object> kafkaParams = new HashMap<>();    kafkaParams.put("bootstrap.servers", "localhost:9092");    kafkaParams.put("key.deserializer", IntegerDeserializer.class);    kafkaParams.put("value.deserializer", StringDeserializer.class);    kafkaParams.put("group.id", "group.spark");    kafkaParams.put("auto.offset.reset", "latest");    kafkaParams.put("enable.auto.commit", false);    SimpleSparkStructuredKafkaStreamingTSP streamingTSPSource = new SimpleSparkStructuredKafkaStreamingTSP(kafkaParams, topic);            // STEP 4. Create the data consumers and register into the algorithm    DataConsumer<AlgorithmObservedData> localDirectoryOutputConsumer =        new LocalDirectoryOutputConsumer<PermutationSolution<Integer>>("outputdirectory");    DataConsumer<AlgorithmObservedData> chartConsumer =        new ChartConsumer<PermutationSolution<Integer>>(algorithm.getName() +"-"+ problem.getName());    // STEP 5. Create the application and run    JMetalSPApplication<        PermutationSolution<Integer>,        DynamicProblem<PermutationSolution<Integer>, ObservedValue<TSPMatrixData>>,        DynamicAlgorithm<List<PermutationSolution<Integer>>, AlgorithmObservedData>> application;    application = new JMetalSPApplication<>();		    application.setStreamingRuntime(new SparkRuntime(5, sparkConf))        .setProblem(problem)        .setAlgorithm(algorithm)        .addStreamingDataSource(streamingTSPSource,problem)        .addAlgorithmDataConsumer(localDirectoryOutputConsumer)        .addAlgorithmDataConsumer(chartConsumer)        .run();  }}